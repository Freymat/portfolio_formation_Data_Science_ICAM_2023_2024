S√©lection des pratiques et comp√©tences acquises lors de ma formation en Data Science √† l'ICAM Strasbourg-Europe, et lors de mes stages et projets personnels.
[La formation en Data Science √† l'ICAM](https://www.icam.fr/formations-professionnelles/formation-data-scientist/) - de l'int√©gration √† l'industrialisation - forme des professionnels capables de r√©pondre aux besoins des entreprises en mati√®re de traitement et d'analyse de donn√©es √† niveau Master.

# ‚õÅ Int√©gration des donn√©es

## ‚õÅ 1. Int√©gration de la base de donn√©es du R√©pertoire National des √âlus (RNE)
### üéØ Objectifs et conduite du projet 
Dans le cadre du projet 'Int√©gration de donn√©es', j'ai travaill√© en √©quipe de trois, sur la r√©alisation d'un ETL (Extract, Transform, Load) pour l'int√©gration de donn√©es provenant de diff√©rentes tables de la base de donn√©es du [R√©pertoire National des √âlus (RNE)](https://www.data.gouv.fr/fr/datasets/repertoire-national-des-elus-1/), 

L'objectif de ce projet √©tait de r√©aliser l'int√©gration d'une base de donn√©es en vue de **r√©pondre √† des requ√™tes de type SQL**, et de produire des rapports statistiques sur les √©lus des diff√©rentes chambres (municipales, d√©partementales, r√©gionales, etc.) en France, d'√©tudier la parit√© au sein des chambres, les nuances politiques des chambres, les mandats cumul√©s, les √©lus les plus √¢g√©s...

Les difficult√©s principales r√©sidaient dans la complexit√© de la base de donn√©es, qui √©tait constitu√©e de plusieurs tables non relationnelles (12 tables ind√©pendantes aux informations r√©p√©titives), et dans la **n√©cessit√© de nettoyer et de transformer les donn√©es pour les rendre exploitables**. En effet, les donn√©es √©taient h√©t√©rog√®nes : les codifications (r√©gions, codes m√©tiers) n'√©taient pas uniformes, certaines √©tant m√™me obsol√®tes. Les champs de dates n'√©taient pas toujours renseign√©s, et il y avait des erreurs de saisie (dates de naissance, nuances politiques, etc.), ainsi que de nombreux doublons.
Il a fallu mettre en place des proc√©dures de nettoyage et de transformation pour les harmoniser.

Une r√©flexion de fond a √©t√© men√©e sur la **mod√©lisation des donn√©es (MCD)** pour d√©terminer la meilleure organisation des tables. Deux approches ont √©t√© envisag√©es :

- Une approche centr√©e sur les chambres : chaque chambre dispose de sa propre table, optimis√©e en externalisant les libell√©s. Pour chaque chambre, les √©lus y sont r√©pertori√©s.
- Une approche centr√©e sur les √©lus : chaque √©lu a sa propre table, indiquant dans quelle chambre il d√©tient un mandat.

L'approche "chambres" a √©t√© retenue pour sa simplicit√©, sa facilit√© de mise en place, d'exploitation et de mise √† jour.

Le nettoyage et la structuration des donn√©es ont √©t√© r√©alis√©s √† l'aide de l'ETL Talend, qui a permis de mettre en place des proc√©dures de nettoyage, de transformation et de chargement des donn√©es.
Les donn√©es ont √©t√© stock√©es dans une base de donn√©es Oracle, h√©berg√©e sur un serveur Linux (Ubuntu). Un serveur Apache, un serveur MariaDB et un serveur PHP ont √©t√© utilis√©s pour la mise en place d'une interface web permettant de consulter les donn√©es . Nous avons utilis√© l'outil DBeaver et phpMyAdmin pour la gestion de la base de donn√©es, et le requ√™tage SQL pour l'interrogation des donn√©es.
Cette solution nous a permis d'acc√©der √† la base de donn√©es de fa√ßon distante (ssh), de donner un acc√®s √† la base de donn√©es √† l'ensemble de l'√©quipe et m√™me au jury le jour de la soutenance. Les requ√™tes ont ainsi pu √™tre ex√©cut√©es en temps r√©el.

Nos requ√™tes SQL ont permis de r√©pondre √† des questions telles que :
- Quelle est la r√©partition des √©lus par genre ?
- Quels sont les √©lus les plus √¢g√©s, les plus jeunes ?
- Quelles sont les chambres les plus √©galitaires, les plus jeunes, les plus √¢g√©es ?
- Quelle est la r√©partition des √©lus par nuances politiques ?
- D'identifier les √©lus cumulant le plus de mandats, les mandats les plus cumul√©s, etc.

Nous avons ainsi cr√©√© un outil s'appuyant sur des donn√©es publiques, qui permet de r√©pondre √† des questions d'int√©r√™t g√©n√©ral sur les √©lus en France, facilement maintenable et √©volutif.

La prochaine √©tape serait la cr√©ation d'un Dashboard pour la visualisation des donn√©es, et la mise en place d'une API pour l'acc√®s aux donn√©es. Notre choix se porterait vers une solution Apache Superset, choix coh√©rent avec notre stack principalement bas√© sur des technologies open source.

### üõ†Ô∏è Moyens et outils
- Mod√©lisation de donn√©es (MCD), logiciel de dessin Dia,
- ETL Talend,
- Serveurs Oracle, OS Linux (Ubuntu), Machine virtuelle, SSH,
- SQL, phpMyAdmin, mariaDB, DBeaver.

üîó Lien vers une pr√©sentation du projet: https://github.com/Freymat/ICAM_Projet_Integration_donnees

## ‚õÅ 2. Nettoyage et pr√©paration des donn√©es issues de l'API de Sefaria.org. Stage
### üéØ Objectifs et conduite du projet

L'objectif de notre stage √©tait la cr√©ation d'un pipeline de donn√©es, permettant la cr√©ation auto-supervis√©e de v√©rit√© de terrain pour l'entra√Ænement des mod√®les de reconnaissance optique de caract√®res (OCR) dans le cadre du projet ERC Synergy MiDRASH (Migrations of Textual and Scribal Traditions via Large-Scale Computational Analysis of Medieval Manuscripts in Hebrew Script).

Le pipeline que nous avons cr√©√© (et qui est d√©taill√© dans la partie ***#REF!!!***) recherche des correspondances entre les transcriptions de milliers de manuscrits et imprim√©s obtenus gr√¢ce √† des mod√®les d'OCR (eScriptorium/Kraken) et des centaines de textes historiques complets provenant de l'API de Sefaria.org. Ces correspondances peuvent alors √™tre utilis√©es comme v√©rit√© de terrain pour l'entra√Ænement des mod√®les d'OCR.

Dans un premier temps, il a fallu collecter un grand nombre de textes complets √† partir de l'API de Sefaria.org, un site web mettant √† disposition un grand nombre de de textes juifs en h√©breu, ainsi que leurs traductions en diff√©rentes langues (Tanakh, Talmud, Midrash, etc.)

Une API permet d'acc√©der √† ces textes (https://developers.sefaria.org/).

Les textes y sont disponibles sous forme de fichiers JSON, et proposent diff√©rents types de structures:
- structure simple: le texte est architectur√© en sections (chapitres), et sous-sections (versets, sous-versets...)
```python
[
  ["Verse 1:1", "Verse 1:2", ...] # Chapter 1
  ["Verse 2:1", "Verse 2:2", ...] # Chapter 2
  [...] 
]
```
- [structure complexe](https://developers.sefaria.org/docs/the-schema-of-a-complex-text), o√π les sections ne pr√©sentent pas toutes la m√™me structure:
```json
{
    "Introduction": ["Intro Paragraph 1", "Intro Paragraph 2", ...],
    "Contents": [
      						["Chapter 1, Section 1", "Chapter 1, Section 2"],
                  ["Chapter 2, Section 1", "Chapter 2, Section 2"], 
                 ...],
    "Conclusion": ["Conclusion Paragraph 1", "Conclusion Paragraph 2", ...]
}
```
La structure de ces textes peut atteindre des degr√©s de complexit√© √©lev√©s, car en outre du texte, ils contiennent par exemple des commentaires:
![alt text](Sefaria/complexe_node_arbanel.png)


*Structure du livre d'Abarbanel sur la Torah (https://developers.sefaria.org/docs/the-schema-of-a-complex-text)*

Nous avons donc mis en place un script Python pour t√©l√©charger ces textes automatiquement, les nettoyer, les concat√©ner, les indexer, et les pr√©parer pour leur utilisation comme v√©rit√© de terrain.

Le d√©fi principal √©tait de produire un code permettant de s'adapter √† tous les types de structure afin de pouvoir parcourir les textes de mani√®re automatique.
Nous avons mis en place des modules de nettoyage des textes permettant de les normaliser (NFKD), de supprimer les signes diacritiques, de supprimer les balises html r√©siduelles, de retirer la ponctuation latine, de supprimer les caract√®res unicodes sp√©ciaux, de supprimer les chiffres arabes...
Les textes ont ensuite √©t√© concat√©n√©s et enregistr√©s dans des fichiers texte. Un indexe de chaque texte a √©t√© cr√©√© afin de pouvoir remonter aux r√©f√©rences des versets dans les textes originaux, en connaissant la position du caract√®re d'un extrait dans le texte complet.

![alt text](Sefaria/index_books.png)

Ce sont plus de 150 textes qui ont √©t√© t√©l√©charg√©s, nettoy√©s, concat√©n√©s et index√©s., pr√™ts √† √™tre utilis√©s comme v√©rit√© de terrain pour la recherche algorithmique d'alignement avec les r√©sultats de l'OCR des manuscrits et imprim√©s. Ce travail est pr√©sent√© ci-dessous (***###REF***)



### üõ†Ô∏è Moyens et outils
- Python, Jupyter notebook, environnement virtuel (conda),
- API Sefaria.org
- Biblioth√®ques Python requests, unicodedata, beautifulsoup, json

üîó Ce travail est disponible sur le d√©p√¥t github: 
https://github.com/Freymat/from_Sefaria_to_Passim



# üîé Analyse des donn√©es
- Python
- statistiques
- ML

## üîé 1. Cr√©ation auto-supervis√©e de v√©rit√© de terrain par m√©thodes algorithmiques

### üéØ Objectifs et conduite du projet
L'objectif premier de mon stage consistait √† mettre en place un pipeline permettant la production de grandes quantit√©s de v√©rit√© de terrain pour l'entra√Ænement des mod√®les de reconnaissance optique de caract√®res (OCR) dans le cadre du projet ERC Synergy MiDRASH (Migrations of Textual and Scribal Traditions via Large-Scale Computational Analysis of Medieval Manuscripts in Hebrew Script).

Pour entra√Æner les mod√®les d'OCR, il est n√©cessaire de disposer de v√©rit√© de terrain, c'est-√†-dire de transcriptions manuelles des textes √† reconna√Ætre. Ces transcriptions peuvent √™tre obtenues par saisie manuelle, mais cela est tr√®s co√ªteux en temps et en ressources humaines.

Nous avons donc mis en place un pipeline de production de v√©rit√© de terrain auto-supervis√©e, qui permet de comparer les transcriptions obtenues par les mod√®les d'OCR (obtenus gr√¢ce aux outils Kraken et eScriptorium) avec des textes complets provenant de l'API de Sefaria.org, et d'en extraire des correspondances. Ces correspondances peuvent alors √™tre utilis√©es comme v√©rit√© de terrain pour l'entra√Ænement des mod√®les d'OCR.

eScriptorium est une plateforme en ligne de gestion de manuscrits et de documents anciens, [d√©velopp√©e par l'√âcole Pratique des Hautes √âtudes (EPHE)](https://classics-at.chs.harvard.edu/classics18-stokes-kiessling-stokl-ben-ezra-tissot-gargem/). Il est bas√© sur le logiciel de reconnaissance optique de caract√®res (OCR) Kraken, qui utilise des r√©seaux de neurones pour la reconnaissance de caract√®res s'adaptant √† un tr√®s grand nombre de scripts.

Le pipeline que nous avons cr√©√© est compos√© de plusieurs √©tapes:
1) Collecte des transcriptions des manuscrits et imprim√©s obtenues par les mod√®les d'OCR (eScriptorium/Kraken).
2) Recherche des correspondances entre les transcriptions et les t√©moins num√©riques (issus de l'API de Sefaria.org) √† l'aide de l'outil de d√©tection d'alignements [Passim](https://github.com/dasmiq/passim), d√©velopp√© par d√©velopp√© par David Smith et son √©quipe dans le but de d√©tecter les cas de r√©utilisation de texte dans un vaste corpus de journaux am√©ricains du XIXe si√®cle projet "Viral Texts" https://viraltexts.org/.  Cet outil s'appuie sur des techniques de traitement automatique du langage naturel (NLP) pour d√©tecter les alignements entre les textes. Dans un premier temps, les textes sont compar√©s en les balayant par fen√™tre de caract√®res (tuilage de n-grammes / w-shingling), puis les alignements sont raffin√©s gr√¢ce √† l'algorithme de Smith-Waterman.
Cet algorithme parcourt chaque document caract√®re par caract√®re et d√©termine si chaque caract√®re d'un document correspond √† un caract√®re de l'autre. Des espaces sont ins√©r√©s l√† o√π les caract√®res ne correspondent pas.
Ci-dessous un exemple d'alignement obtenu par Passim, entre une ligne d'OCR et un t√©moin num√©rique. En rouge les caract√®res qui manquent dans l'OCR, en vert les caract√®res en trop par rapport au t√©moin num√©rique.
![alt text](pipeline_stage/alignement_3.png)
Finalement, les alignements retenus sont ceux qui ont un ratio de Levenshtein sup√©rieur √† un seuil fix√© par l'utilisateur.
3) Extraction des alignements d√©tect√©s par Passim, et cr√©ation d'une nouvelle transcription. Pour chaque feuillet de document, les alignements retenus sont int√©gr√©s dans un fichier xml alto (Analytical Layout and Text Object), qui permet de stocker les transcriptions et les alignements.
4) Export dans eScriptorium pour la visualisation des alignements, et l'entra√Ænement des mod√®les.

Outre l'import dans eScriptorium des alignements pour la visualisation et l'entrainement des mod√®les, nous avons √©galement cr√©√© des dataframes de synth√®se des alignements permettant d'identifier pour chaque feuillet "oc√©ris√©", les meilleurs t√©moins num√©riques align√©s, ainsi que des dictionnaires contenant les informations sur les alignements (texte, position, score de confiance).
Tous ces outils sont d√©crits sur le d√©p√¥t github du projet: https://github.com/Freymat/from_eScriptorium_to_Passim_and_back

Le pipeline se pr√©sente sous la forme d'un script python, qui peut √™tre d√©ploy√© sur diff√©rents types de machines (ordinateurs personnels, clusters de calcul). La proc√©dure d'installation est d√©crite.  Il a √©t√© test√© sur des ordinateurs personnels, des machines virtuelles, et des clusters de calcul (cluster de calcul de l'in2p3). Il est actuellement en cours de d√©ploiement sur le cluster de calcul du projet MiDRASH.

Actuellement, nous utilisions le pipeline sur des batchs de 7000 manuscrits que nous croisons avec plus de 150 textes num√©rique. 

**Travaux pour la suite:**
- Diminuer le temps de calcul lors de la cr√©ation des fichiers .tsv de synth√®se des r√©sultats, en parall√©lisant le traitement des jsons (avec apache spark par exemple).
- Analyse fine des clusters de lignes align√©es successivement, afin d'identifier les erreurs les plus fr√©quentes du mod√®le d'OCR (tableau de confusion).








### üõ†Ô∏è Moyens et outils
- Python, Jupyter notebook, environnement virtuel (conda)
- outils git et github pour la gestion de version
- eScriptorium/ API eScriptorium, Kraken pour l'OCR des manuscrits et imprim√©s
- Passim pour la d√©tection d'alignements (NLP/Algorithmes de d√©tection d'alignements)

üîó Lien vers le code du pipeline ainsi que sa documentation: https://github.com/Freymat/from_eScriptorium_to_Passim_and_back


## üîé 2. Analyse du contenu de la base de donn√©es openfoodfacts.
### üéØ op√©rations  et conditions:     
Dans le cadre du projet 'Data Analyst: de l'int√©gration √† l'industrialisation', j'ai travaill√© sur l'analyse des donn√©es de la base de donn√©es Openfoodfacts, √©tape indispensable √† la r√©alisation de la classification des allerg√®nes dans les produits alimentaires.
Nous d√©taillons au prochain point (***###REF***) la classification des allerg√®nes dans les produits alimentaires de la base de donn√©es Openfoodfacts. Ici nous nous concentrons sur l'analyse des donn√©es de la base de donn√©es.

Cette partie d'exploration nous a permis d'en comprendre le contenu, la structure et la qualit√©. Mais surtout de **construire avec la di√©t√©ticienne nutritionniste** le projet d'application en lui exposant le contenu de la base, et d'en estimer la faisabilit√© sur la base des donn√©es disponibles.

#### Contenu de la Base de Donn√©es :

Open Food Facts est une base de donn√©es ouverte et collaborative qui contient des informations sur des produits alimentaires du monde entier.
√Ä la date de l'√©tude (f√©vrier 2024), la base comprenait 3,069,472 lignes (produits) et 206 colonnes couvrant divers aspects des produits alimentaires, tels que la composition, la provenance, la marque, l'emballage, les cat√©gories, les labels, les additifs, les allerg√®nes, les nutriments, etc.

#### Sources de Donn√©es :
Les donn√©es proviennent de contributions individuelles via des applications mobiles (Open Food Facts, Yuka, Foodvisor) ainsi que d'institutions telles que le minist√®re de l'Agriculture des √âtats-Unis (USDA) et l'√âcole polytechnique f√©d√©rale de Lausanne (EPFL).
Les ajouts ponctuels et massifs de donn√©es de ces deux derni√®res institutions sont bien visibles dans la base de donn√©es.
L'analyse des contributions a r√©v√©l√© une croissance constante jusqu'en 2022, suivie d'une stagnation et d'une l√©g√®re d√©croissance depuis 2023.
#### Qualit√© des Donn√©es :

##### Compl√©tion des Donn√©es :
Une analyse pr√©liminaire a montr√© que 50% des colonnes contiennent moins de 1% d'enregistrements compl√©t√©s.
En moyenne, les lignes de la base de donn√©es sont compl√©t√©es √† 20%, ce qui indique un nombre significatif de valeurs manquantes.
Colonnes Cl√©s :
Les colonnes les plus compl√©t√©es sont celles contenant des informations obligatoires comme le nom, la marque, le code-barres, et le Nutri-Score.
Les colonnes contenant des informations facultatives ou plus complexes √† renseigner, telles que les additifs, les allerg√®nes, et les nutriments, sont souvent incompl√®tes. Par exemple, les allerg√®nes ne sont renseign√©s que pour 8.06% des produits.

##### Visualisation des Donn√©es Manquantes :
Utilisation de la biblioth√®que missingno pour visualiser les valeurs manquantes.
Cette visualisation a r√©v√©l√© des lacunes significatives dans la compl√©tion des donn√©es. Les graphiques montrent que de nombreuses colonnes sont incompl√®tes, avec des lignes blanches repr√©sentant les valeurs manquantes.

##### Probl√®mes Structurels :
La base de donn√©es pr√©sente des duplications de colonnes, par exemple, certaines informations sont pr√©sentes sous plusieurs formes (e.g., 'x', 'x_tags', 'x_en').
Certaines lignes sont quasi vides, probablement en raison d'une interruption de la saisie par les utilisateurs.
Les champs 'url' sont lourds et repr√©sentent 13% du volume total de la base de donn√©es, ce qui pourrait √™tre optimis√© pour r√©duire la taille globale.
#### Optimisation de la Base de Donn√©es :
Une r√©organisation de la structure de la base de donn√©es pourrait am√©liorer l'efficacit√© et r√©duire le volume des donn√©es.
Pour une utilisation en SQL, il serait avantageux de subdiviser la base en plusieurs tables regroup√©es autour d'une table d'identification (id).

#### Conclusion de cette partie:
Pour les besoins de notre projet nous avons du ensuite s√©lectionner les colonnes pertinentes pour la classification des allerg√®nes, et nettoyer les donn√©es pour les rendre exploitables. Cette partie est d√©taill√©e dans le point suivant (***###REF***).



### üõ†Ô∏è Moyens et outils


- pandas, jupyter notebook, conda
- missingno, matplotlib


üîó Lien vers une pr√©sentation du projet: https://github.com/Freymat/ICAM_Projet_Data_Science

## üîé 3. Classification d'allerg√®nes dans les produits alimentaires de la base de donn√©es openfoodfacts.
Dans le cadre du projet 'Data Scientist: de l'int√©gration √† l'industrialisation'.
Ce travail est expos√© en d√©tail dans le repository github du projet: https://github.com/Freymat/ICAM_Projet_Data_Science

### üéØ Objectifs et conduite du projet
Ce projet individuel a √©t√© r√©alis√© en 10 jours dans le cadre de ma formation en Data Science √† l'ICAM . L'objectif √©tait la conception d'une application au service de la sant√© publique, en proposant une id√©e innovante d‚Äôapplication en lien avec l‚Äôalimentation pour l‚Äôagence Sant√© Publique France. √Ä notre disposition, la base de donn√©es Openfoodfacts.

Apr√®s avoir interview√© une di√©t√©ticienne nutritionniste afin d'identifier comment une application bas√©e sur les donn√©es de la base Openfoodfacts pourrait r√©pondre aux besoins et aux attentes des professionnels de sant√©, nous avons choisi de travailler sur la classification des allerg√®nes dans les produits alimentaires.
(La partie analyse des donn√©es de la base de donn√©es Openfoodfacts est d√©crite ci-dessus ***# REF!!!***).

L'enqu√™te de terrain a orient√© nos travaux vers la conception d'un assistant num√©rique qui servirait de trait d'union entre le professionnel et le patient, facilitant ainsi la mise en ≈ìuvre du r√©gime alimentaire prescrit. Selon elle, un tel assistant pourrait offrir deux fonctionnalit√©s principales :

- Scanner un produit alimentaire pour informer le patient sur la mani√®re dont il peut s'int√©grer dans le r√©gime prescrit : dans quel repas de la semaine, en quelle quantit√©, avec quel autre aliment d'accompagnement, et dans quelle recette il peut √™tre consomm√© ; comment ajuster le programme alimentaire de la semaine pour l'√©quilibrer en fonction de ce produit.

- Scanner un produit alimentaire pour alerter sur le risque de pr√©sence d'un allerg√®ne, m√™me si celui-ci n'est pas mentionn√© sur l'√©tiquette ou si la composition est r√©dig√©e en langue √©trang√®re.

C'est sur ce deuxi√®me point qu'a port√© notre travail.

Notre objectif est de concevoir une application capable de pr√©dire la pr√©sence d'allerg√®nes dans un produit alimentaire en scannant simplement son code-barre.

L'exploration de la base de donn√©es a r√©v√©l√© que les allerg√®nes ne sont renseign√©s que pour 8,06% des produits, rendant impossible l'utilisation directe de cette colonne 'allerg√®nes' pour informer les patients.

Nous avons donc d√©cid√© de tenter de classifier les allerg√®nes en utilisant les autres informations disponibles dans la base de donn√©es, comme les ingr√©dients par exemple.

Pour valider notre approche, nous avons choisi de nous concentrer sur un allerg√®ne sp√©cifique : le gluten. Nous avons construit un jeu de donn√©es binaire indiquant la pr√©sence de gluten : 1 si le gluten est pr√©sent dans la liste des allerg√®nes, 0 sinon.

L'objectif √©tait donc d'entra√Æner un mod√®le de classification capable de pr√©dire la pr√©sence de gluten dans un produit alimentaire √† partir de sa liste d'ingr√©dients.

 Nous avons selectionn√© trois architectures de classifieurs, de la plus simple √† la plus √©labor√©e: un arbre de d√©cision, un perceptron multicouche (MLPClassifier) et des r√©seaux de neurones r√©currents LSTM (uni et bidirectionnels).
 
#### Pr√©paration des donn√©es

Nous avons travaill√© sur un dataset restreint aux donn√©es pertinentes pour notre √©tude :

S√©lection des colonnes se limitant aux ingr√©dients, cat√©gories d'aliments et allerg√®nes.
S√©lection des lignes o√π la colonne 'allerg√®nes' est renseign√©e.
Cr√©ation d'une colonne binaire 'gluten_presence' (1 si gluten pr√©sent, 0 sinon).
Le dataset final contient 27 colonnes et :

106 752 lignes pour la classe 1 (gluten pr√©sent).
137 276 lignes pour la classe 0 (gluten absent).
Les classes sont relativement √©quilibr√©es, ce qui est important pour la conception du classificateur.

Quel que soit le mod√®le de classification utilis√©, le principe reste le m√™me :

- Pr√©paration des donn√©es textuelles.
- S√©paration des donn√©es en ensembles d'entra√Ænement et de test.
- Entra√Ænement du mod√®le.
- √âvaluation du mod√®le.

#### R√©sultats des Mod√®les de Classification

##### Arbre de D√©cision

Pour utiliser les mod√®les d'arbre de d√©cision, les donn√©es textuelles ont √©t√© transform√©es en donn√©es num√©riques via des techniques de NLP :
- **Nettoyage, tokenisation, et vectorisation** des donn√©es textuelles avec `nltk` et `CountVectorizer` de `sklearn`.
- Utilisation des **n-grammes** pour capturer des s√©quences de mots (ex. : "sans gluten" vs. "gluten free").

Apr√®s optimisation des hyperparam√®tres avec `GridSearchCV`, l'arbre de d√©cision a obtenu un score F1 de **0.87** et un recall de **0.92** pour la classe 0 (absence de gluten).

##### Perceptron Multicouche (MLP Classifier)

En utilisant une m√©thodologie similaire √† celle de l'arbre de d√©cision :
- Le **MLP classifier** a √©galement atteint un score F1 de **0.87** et un recall de **0.91** pour la classe 0.

##### Conclusion pour les Mod√®les de Classification Simples

- Les r√©sultats sont satisfaisants mais doivent √™tre am√©lior√©s pour √©viter les **faux n√©gatifs**, dangereux pour les allergiques.
- Les mod√®les ne g√®rent pas bien les s√©quences de mots, probl√®me partiellement contourn√© par la cr√©ation de tokens sp√©cifiques (ex. : "glutenfree").

#### R√©seaux de Neurones R√©currents LSTM

Les **LSTM** sont adapt√©s pour traiter des s√©quences de mots gr√¢ce √† leur capacit√© √† m√©moriser les informations pass√©es :
- Utilisation d'une **architecture simple** avec une couche d'**embedding**, plusieurs couches **LSTM**, des couches de **dropout**, et une couche **dense** de sortie.
- Test des **couches bidirectionnelles** pour am√©liorer les r√©sultats.

R√©sultats des LSTM:

- Les **courbes d‚Äôentra√Ænement et de validation** ont montr√© une bonne capacit√© de g√©n√©ralisation du mod√®le sans surapprentissage notable.
- Les r√©sultats sont tr√®s satisfaisants avec un **recall** allant jusqu'√† **0.94** pour la classe 0.
- Le mod√®le bidirectionnel a fourni les meilleurs r√©sultats avec un vocabulaire de base de **5000 mots**, ce qui en fait notre mod√®le pr√©f√©r√©.

##### Conclusion G√©n√©rale

- L'arbre de d√©cision et le perceptron multicouche ont donn√© des r√©sultats acceptables mais insuffisants pour une mise en production.
- Les r√©seaux **LSTM** ont montr√© de meilleures performances, particuli√®rement le mod√®le bidirectionnel avec un vocabulaire optimis√©.
- De plus, le mod√®le **LSTM capture mieux les s√©quences de mots** (ex. : "sans gluten" n'a pas le m√™me sens que "gluten free").

### üõ†Ô∏è Moyens et outils
- Machine Learning
- Python, Jupyter notebook, environnement virtuel (conda)
- pandas, matplotlib
- nltk, scikit-learn, tensorflow

üîó Pour rappel: l'ensemble des travaux est disponible sur le d√©p√¥t github du projet (rapport, notebooks, pr√©sentation): https://github.com/Freymat/ICAM_Projet_Data_Science

Outils utilis√©s:

- python, conda
- jupyter notebook, pandas, numpy, nltk, 
- scikit-learn, tensorflow





https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset
Default of Credit Card Clients Dataset
Default Payments of Credit Card Clients in Taiwan from 2005
D√©marche de data story telling


# üìä Visualisation des donn√©es
- techno WEB
- Chatbot, API IBM
- Tableau/PowerBI

## üìä 1. Projet Data Visualisation: Dashboard VOD avec Tableau.
Dans le cadre du projet 'Visualisation des donn√©es', j'ai travaill√© √† la r√©alisation d'un tableau de bord interactif sur les plateformes de vid√©o √† la demande (VOD).

 L'objectif √©tait la cr√©ation d'un dashboard interactif avec le programme Tableau de Salesforce. Apr√®s **exploration, nettoyage et int√©gration**, les donn√©es devaient √™tre **visualis√©es** dans un tableau de bord interactif.

Le dataset qui nous a √©t√© fourni √©tait constitu√© de fichiers .csv, contenant des informations sur les films et s√©ries disponibles sur les plateformes de vid√©o √† la demande (VOD) Netflix, Amazon Prime Video, Disney+ et Hulu.

J'ai choisi d'utiliser ces donn√©es pour cr√©er un tableau de bord permettant √† chacun de comparer le contenu des quatre plateformes de vid√©o √† la demande et de choisir celle qui correspond le mieux √† ses pr√©f√©rences.
Plut√¥t que de submerger l'utilisateur avec une multitude d'indicateurs cl√©s (KPIs), nous avons adopt√© une approche ax√©e sur la simplicit√© et la clart√©.
Nous avons recherch√© les indicateurs les plus pertinents pour aider l'utilisateur √† prendre une d√©cision √©clair√©e. Le design √©pur√© du tableau de bord a √©t√© con√ßu pour faciliter la prise de d√©cision, en mettant l'accent sur les √©l√©ments visuels et interactifs tels que les couleurs et les survols de souris (hoovers).
C'est cette qu√™te de clart√© et d'√©pure qui nous a motiv√©s, et nous esp√©rons y √™tre parvenus au moins en partie.

Lors de la pr√©sentation, nous avons mis en oeuvre une **d√©marche de type 'storytelling'** en pr√©sentant l'outil comme **une aide intuitive √† la d√©cision**, √† destination du grand public.

üîó Le tableau de bord est disponible en ligne sur le site de Tableau Public :
https://public.tableau.com/app/profile/m.freyder/viz/VOD_17163198511630/Tableaudebord1

Pr√©sentation du projet: https://github.com/Freymat/ICAM_Projet_Data_VisualisationD
## üìä 2. Visualisation de donn√©es textuelles
La visualisation de donn√©es textuelles a √©t√© une composante de mon travail lors de mon stage √† l'EPHE.
L'objectif des mod√®les de segmentation et d'OCR est de transformer des images de manuscrits et d'imprim√©s en texte brut, afin de faciliter leur √©tude et leur analyse.

Afin d'entra√Æner les mod√®les d'OCR, nous avons donc travaill√© √† la production de grandes quantit√©s de v√©rit√© de terrain. Celle-ci est obtenue par comparaison des transcriptions des mod√®les d'OCR avec des t√©moins num√©riques. A diff√©rentes √©tapes du pipeline, il est n√©cessaire de visualiser les alignements entre les transcriptions et les t√©moins num√©riques, afin de v√©rifier la qualit√© des r√©sultats et de les corriger si n√©cessaire.

Nous avons utilis√© deux outils principaux pour la visualisation des donn√©es textuelles :
- **1) cr√©ation de dictionnaires (json)** synth√©tisant les alignements d√©tect√©s par Passim, pour chaque feuillet de document, et chaque t√©moin num√©rique. Ces dictionnaires contiennent les informations sur les alignements (texte, position, score de confiance) et sont utilisables pour l'entra√Ænement des mod√®les d'OCR.

![alt text](image-4.png)

Dans cet exemple, chaque dictionnaire contient les informations sur une ligne d'OCR identifi√©e par son id. Il contient notamment le texte de l'OCR, la position du texte dans le document, la valeur du ratio de Levenshtein, et le texte du t√©moin num√©rique align√©. On visualise ainsi les diff√©rences entre l'OCR (cl√© `text`) et le t√©moin num√©rique (cl√© `alg_GT`).

- **2) eScriptorium** : la plateforme en ligne eScriptorium permet ce type de visualisation de fa√ßon plus 'user-friendly'.
Notre pipeline permet la cr√©ation de fichiers xml alto, contenant les transcriptions bas√©es sur les alignements d√©tect√©s par Passim et retenus. Ces fichiers peuvent √™tre import√©s dans eScriptorium pour la visualisation des alignements, et l'entra√Ænement des mod√®les.

L'image ci-dessous montre les r√©sultats de l'alignement d'un texte (siddur ashkenaz) sur les les lignes d'OCR d'un manuscrit.
La page de gauche est un scan d'un manuscrit. Le mod√®le de segmentation a identifi√© les lignes de texte et les r√©gions de la page. La page de droite visualise la transcription contenant les alignement du Siddur Ashkenaz sur les lignes d'OCR.
![alt text](visualisation_stage/siddur.png)
7 lignes ont ainsi √©t√© align√©es. Le contenu de ces lignes est pr√©sent sur la page de droite, et sont entour√©s de vert. Les zones vides correspondent aux lignes non align√©es.
Il est possible d'√©tudier en d√©tail les diff√©rences entre l'OCR et l'alignement, en cliquant sur les lignes.

![alt text](visualisation_stage/transcription_comparison_2.png)

Avec:
1. le scan du manuscrit, avec en surbrillance les polygones de segmentation des lignes de texte.
2. la transcription contenant les alignements du Siddur Ashkenaz sur les lignes d'OCR.
Dans le champ 'Toggle transcription comparison' on peut visualiser les diff√©rences entre l'OCR et l'alignement:
3. le contenu de l'OCR
4. le contenu de l'alignement
En vert, les caract√®res en plus dans dans l'OCR, en rouge les caract√®res manquants. On voit ainsi que le mot '◊ë◊™◊ï◊®◊™◊ö' est pr√©sent dans l'OCR, mais pas dans l'alignement.
Autre exemple: 

![alt text](visualisation_stage/transcription_comparison_3.png)

**Int√©r√™t de la visualisation des donn√©es textuelles**: Visualiser les alignements m'a permis de v√©rifier les r√©sultats de mon pipeline. En effet, ce pipeline traitant des milliers de pages de documents, il est important de pouvoir v√©rifier visuellement la qualit√© de l'alignement, en particulier dans la phase de d√©veloppement du code.
### üõ†Ô∏è Moyens et outils
- Python, xml, json
- eScriptorium, Passim


# üè≠ Industrialisation des donn√©es
- RGPD
- Gestion de projet informatique
- 

## üè≠ 1. Mise en production d'un pipeline de production auto-supervis√©e de v√©rit√© de terrain 
A l'issue de la phase de d√©veloppement du pipeline de production de v√©rit√© de terrain, nous avons travaill√© √† sa mise en production.

Apr√®s avoir d√©velopp√© le code du pipeline de fa√ßon modulaire dans jupyter notebook, nous avons proc√©d√© √† sa transformation en script python, afin de pouvoir le d√©ployer sur diff√©rents types de machines (ordinateurs personnels, clusters de calcul).

Le code a √©t√© revu par les pairs (d√©veloppeur full stack senior), et des tests unitaires ont √©t√© r√©alis√©s pour v√©rifier son bon fonctionnement. Nous avons cherch√© √† optimiser le code pour r√©duire les temps de calcul. L'un des gains les plus cons√©quents a √©t√© obtenu en rempla√ßant la biblioth√®que lxml par des expressions r√©guli√®res pour le traitement des fichiers xml (insertion des alignements dans les fichiers alto), passant de plusieurs heures √† quelques minutes pour le traitement de 900 manuscrits sur un ordinateur personnel.

Les outils de gestion de version git et github ont √©t√© utilis√©s pour la gestion de version du code.

Le pipeline a √©t√© d√©ploy√© sur le cluster de calcul de l'in2p3, sur lequel nous avons op√©r√© des tests grandeur nature.

Notre objectif √©tant de fournir au projet MiDRASH un outil utile et fonctionnel pour la production de v√©rit√© de terrain nous avons veill√© √† ce que le code soit bien document√©. Le code est disponible sur le d√©p√¥t github, avec un guide d'utilisation et une documentation compl√®te.

Enfin, un accompagnement a √©t√© dispens√©e aux autres membres de l'√©quipe pour leur expliquer le fonctionnement du pipeline et les former √† son utilisation.


### üõ†Ô∏è Moyens et outils
- calculateurs en environement Linux
- python, Jupyter notebook, environnement virtuel (conda)
- eScriptorium, Kraken, Passim
- git, github

Lien vers le projet (et ses branches de d√©veloppement) : https://github.com/Freymat/from_eScriptorium_to_Passim_and_back


## üè≠ 2. √âtude comparative de solutions de bases de donn√©es pour la mise en production de notre application √† destination des professionnels de sant√©.

### üéØ Objectifs et conduite du projet
Dans le cadre du projet 'Data Scientist: de l'int√©gration √† l'industrialisation', nous avons travaill√© avec une di√©t√©ticienne afin de cr√©er un assistant num√©rique pour l'aide au suivi des r√©gimes alimentaires prescrits aux patients. Ce projet a d√©bouch√© sur la cr√©ation d'une application de classification des allerg√®nes dans les produits alimentaires, bas√©e sur les donn√©es de la base Openfoodfacts.

Lors de la phase de conception de l'application, nous avons avons anticip√© la mise en production de l'application, afin de nous assurer de la faisabilit√© technique du projet. La base de donn√©es Openfoodfacts est-elle adapt√©e √† notre application ? Quelle solution de base de donn√©es est la plus adapt√©e √† notre projet ?

En effet la base de donn√©es Openfoodfacts est volumineuse (3,069,472 lignes et 206 colonnes en f√©vrier 2024) nous incitant √† r√©fl√©chir √† la meilleure solution de base de donn√©es pour la mise en production de notre application.

Nous avons choisi de comparer les bases de donn√©es SQL (MariaDB) et NoSQL (MongoDB) pour la mise en production de notre application fonctionnant avec la base de donn√©e openfoodfacts.

#### Solution Maria DB (SQL)

Nous avons envisag√© de r√©importer la base de donn√©es sous forme de tables SQL afin de pouvoir la requ√™ter pendant la phase de d√©veloppement et pourquoi pas de mise en production de notre application. La cr√©ation de la structure de la table √©tait tr√®s fastidieuse en raison du grand nombre de colonnes et de la vari√©t√© des donn√©es. Nous avons du d√©finir de fa√ßon pr√©cise le type des donn√©es de chaque colonne et adapter la longueur des champs pour chaque type de donn√©es. Nous avons √©cris un script python pour la cr√©ation de la base de donn√©es et des tables, et l'import des donn√©es gr√¢ce √† la biblioth√®que sqlalchemy (pour la gestion des requ√™tes SQL de cr√©ation de la table). Malheureusement, les essais d'import de la base en local ont √©chou√© en raison de la taille de la base impossible √† charger en entier. Un important travail de restructuration de la base de donn√©es aurait √©t√© n√©cessaire, en la scindant en plusieurs tables, en triant et en nettoyant les donn√©es, avec la conception d'un ETL pour la mise √† jour r√©guli√®re des donn√©es, ce qui n'√©tait pas envisageable dans le cadre de notre projet.
Cette solution n'a donc pas √©t√© retenue, mais nous sommes heureux d'avoir pu explorer cette solution SQL et identifier les avantages et les inconv√©nients de cette solution.

![alt text](sql_mongo/SQL.PNG)

Nous avons donc ensuite investigu√© la solution MongoDB.

#### Solution MongoDB (NoSQL)
 La base de donn√©es Open Food Facts √©tant d√©j√† disponible sous forme de dump MongoDB, cette solution a trouv√© notre faveur pour plusieurs raisons:
- plus adapt√©e aux donn√©es textuelles que SQL,
- scalabilit√© et haute disponibilit√© des bases de donn√©es NoSQL,
- toutefois son mod√®le de donn√©es flexible (pas de sch√©ma pr√©d√©fini) n'encourage pas √† la structuration des donn√©es. La base de donn√©es souffrant d√©j√† d'un manque de structuration, nous ne pourrons faire l'√©conomie d'un travail de nettoyage et de restructuration de la base en vue de la rendre plus l√©g√®re.

Afin de tester cette solution, nous avons fait tourner la base de donn√©es MongoDB en local sur un syst√®me Linux. Nous avons requ√™t√© et explor√© la base de donn√©e √† l'aide de Compass, l'interface graphique de MongoDB.
C'est cette solution qui a √©t√© retenue pour la mise en production future de notre application.

![alt text](sql_mongo/mongoDB.PNG)

### üõ†Ô∏è Moyens et outils
- SQL: serveur Apache, phpMyAdmin, MariaDB, Python, Jupyter Notebook, biblioth√®que sqlalchemy
- NoSQL: MongoDB, Compass

Pr√©sentation et notebook disponibles sur le d√©p√¥t github du projet:
https://github.com/Freymat/ICAM_Projet_Data_Science/tree/main
https://github.com/Freymat/ICAM_Projet_Data_Science/tree/main/working_notebooks

## üè≠ 3. Conception d'un chatbot documentaire.
### üéØ Objectifs et conduite du projet

Dans le cadre du projet IA/Watson de notre formation √† l'ICAM, nous avons travaill√© sur la conception d'un chatbot documentaire, en utilisant les outils de la plateforme IBM Watson.
Nous avons appliqu√© cet outil √† une usine de fabrication de roulements √† bille.

Le chatbot avait pour r√¥le principal de guider les collaborateurs d'une entreprise sur leur intranet (authentification, ouverture de tickets en cas de probl√®mes informatiques, etc.). En connectant le chatbot √† la base de donn√©es documentaire de l'entreprise, nous avons facilit√© l'acc√®s des collaborateurs aux informations n√©cessaires via des questions simples pos√©es au chatbot.

Le chatbot a ainsi √©t√© connect√© √† une base de donn√©es documentaire factice contenant:
- Donn√©es techniques sur les produits de l'entreprise : Types de roulements √† bille, leurs caract√©ristiques.
- Proc√©dures de fabrication : Normes de qualit√©, certifications.
- Proc√©dures de maintenance : Pi√®ces d√©tach√©es, garanties.
- Proc√©dures de recyclage, normes environnementales.
- Informations sur l'entreprise : Annuaire des collaborateurs, organigramme.

Ayant constat√© que les documents produits lors des diff√©rentes √©tapes d'un projet sont rarement exploit√©s (rapports finaux, retours d'exp√©rience), nous avons con√ßu ce chatbot documentaire pour faciliter l'acc√®s √† ces documents archiv√©s, en vue de leur r√©utilisation dans de futurs projets.

Exemple de questions que nous pouvons poser √† notre chatbot documentaire (la documentation est en anglais) :
	 	 	 	
**Retours d'exp√©rience:**
"Where was the last Experience Feedback Meeting held?"
"What are the lessons learned?"
"What problems were encountered?"

**Question sur projets en cours:**
"What is the start date of project ‚ÄòDesign of the Latest Brand Ball Bearing‚Äô?"
"What are the project constraints?"

**Questions sur un produit:**
"What are the Product Specifications of the Innovative Ball Bearing ‚Äì BX100?"
"What are the dimensions of the BX100?"
"What are the use cases of the BX100?"
"What is the warranty period of the BX100?"

**Annuaire / Organigramme :**
"Who is the Head of the Research and Development Department?"
"Who are the members of the Research and Development Department?"
"What is the phone number of the Head of the Research and Development Department?"
"In which building/office is Mr. John Doe located?"

Ce projet a fait l'objet d'une pr√©sentation et d'une d√©mo live lors de la soutenance finale du projet IA/Watson.

### üõ†Ô∏è Moyens et outils
Nous avons utilis√© la plateforme IBM Watson X :

- Watson Assistant : Pour la cr√©ation du chatbot.
- Watson Discovery : Pour l'indexation et la recherche des documents.


